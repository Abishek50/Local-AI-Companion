#Overview

This project implements a fully local AI conversational system combining:

ğŸ¦™ Local LLM inference using llama.cpp
ğŸŒ Web-based chat interface via SillyTavern
ğŸ”Š Real-time voice output using GPT-SoVITS v2 / XTTS

The system runs entirely offline and supports GPU acceleration for efficient inference.
